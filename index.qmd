---
title: "Intro to Bayes - a one day workshop"
format: html
editor: 
  markdown: 
    wrap: 72
---

<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<img src="misc/banner_image1.png" alt="AI generated banner image" class="center" width="100%"/>

<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/mattiasvillani/introbayes" data-color-scheme="no-preference: dark; light: light; dark: dark;" data-size="large" data-show-count="true" aria-label="Star mattiasvillani/introbayes on GitHub">Star</a>

### Aim

The aim of this one-day workshop is to introduce:

- the basics of Bayesian statistics though some simple models
- the Bayesian approach to prediction and decision making
- approximation and sampling algorithms for posterior inference
- Bayesian regression and classification 
- probabilistic programming in Stan and Turing that will allow the user to tackle serious real-world problems with ease.

The treatment of each topic will necessarily be more brief than I would like to, but the partipants can dig deeper by:

- reading in the book suggested below

- experiment with the [extra material](#extras) provided below

- follow along in the material for my two courses:
  - [Bayesian Learning. 7.5 credits](https://github.com/mattiasvillani/BayesLearnCourse) 
  - [Advanced Bayesian Learning, 8 credits](https://github.com/mattiasvillani/AdvBayesLearnCourse) 

### Lecturer

<img src="/misc/villani_foto.jpg" width="15%"/>\
[Mattias Villani](https://mattiasvillani.com)<br> Professor of
Statistics<br> Stockholm University

### Literature

-   Villani, M. (2025). [Bayesian Learning](https://github.com/mattiasvillani/BayesianLearningBook/raw/main/pdf/BayesBook.pdf) (draft, later chapters are work in progress)

### Workshop plan and schedule

---

**Lecture 1 - The Bayesics**<br>
Time: 9.00-9.50 <br>
Reading: Ch. 1, 2.1-2.4 | [Slides](slides/L1.pdf) <br>
Interactive: 
[Beta distribution](https://observablehq.com/@mattiasvillani/beta-distribution) |
[Bernoulli data - beta prior](https://observablehq.com/@mattiasvillani/bayesian-inference-for-bernoulli-iid-data) | 
[Gaussian known variance](https://observablehq.com/@mattiasvillani/bayes-iid-gaussian-known-var) |
[Poisson model](https://observablehq.com/@mattiasvillani/bayesian-inference-for-iid-poisson-counts) | 
[Exponential model](https://observablehq.com/@mattiasvillani/bayesian-inference-for-exponential-iid-data) |
[Credible intervals](https://observablehq.com/@mattiasvillani/bayesian-credible-intervals)

**Lecture 2 - Multi-parameter models, Marginalization, Prior elicitation and Prediction**<br> 
Time: 10.00-10.50 <br>
Reading: Ch. 3.1-3.5, Ch.4 and Ch. 6 | [Slides](slides/L2.pdf)<br> 
Interactive: [Scaled inverse-chi2 distribution](https://observablehq.com/@mattiasvillani/scaled-inverse-chi-2-distribution) | 
[Gaussian model](https://observablehq.com/@mattiasvillani/bayesian-inference-for-gaussian-iid-data) | 
[Dirichlet distribution](https://observablehq.com/@mattiasvillani/dirichlet-distribution) | 
[Multinomial model](https://observablehq.com/@mattiasvillani/multinomial-dirichlet) | 
[Prior predictive Poisson model](https://observablehq.com/@mattiasvillani/prior_pred_poismodel)

‚òï *coffee*

**Lecture 3 - Bayesian Regression and Regularization**<br> 
Time: 11.10-12.00 <br>
Reading: Ch. 5 and Ch. 12 | [Slides](slides/L3.pdf)<br> 
Interactive: [Linear regression](https://observablehq.com/@mattiasvillani/bayesian-linear-regression-bike-share-data)

üç≤ *lunch* 

**Lecture 4 - Bayesian Classification and Posterior Approximation**<br> 
Time: 13.00-13.50 <br>
Reading: Ch. 7 and Ch. 8 | [Slides](slides/L4.pdf)<br>
Interactive: [Beta regression for proportions](https://observablehq.com/@mattiasvillani/posterior-approximation-beta-model)<br>
Notebook: Logistic regression Titanic data in R:  [html](https://mattiasvillani.com/BayesianLearningBook/notebooks/TitanicLogistic/TitanicLogistic.html) and [quarto](https://github.com/mattiasvillani/BayesianLearningBook/raw/main/notebooks/TitanicLogistic/TitanicLogistic.qmd)

**Lecture 5 - Introduction to Gibbs sampling, MCMC and HMC**<br> 
Time: 14.00-14.50 <br>
Reading: Ch. 9 and Ch. 10 (very incomplete) | [Bayesian Data Analysis Ch. 10-11 and 12.4](https://sites.stat.columbia.edu/gelman/book/BDA3.pdf) | [Slides](slides/L5.pdf)<br>
Interactive:  [Random walk Metropolis](https://observablehq.com/@mattiasvillani/random-walk-metropolis) | [HMC](https://observablehq.com/@mattiasvillani/hamiltonian-markov-chain-monte-carlo) | [Leapfrog integrator](https://observablehq.com/@mattiasvillani/leapfrog-integrator) | [HMC on multimodal posterior](https://observablehq.com/@mattiasvillani/hmc-sampling-from-multi-modal-distributions)<br>
Videos: [Gibbs sampling when parameters are very correlated](https://www.youtube.com/watch?v=IGiQOCX9UbM&ab_channel=Red15) | [Metropolis-Hastings vs HMC](https://www.youtube.com/watch?v=Vv3f0QNWvWQ&ab_channel=DavidDuvenaud)

‚òï *coffee*

**Lecture 6 - Implementing Bayesian Learning with Probabilistic Programming**<br> 
Time: 15.10-16.00 <br>
Reading: Ch. 1, 2.1-2.4 | [Slides](slides/L6.pdf)<br> 
Notebooks: Polynomial regression for fossil data in Rstan [html](https://mattiasvillani.com/BayesianLearningBook/notebooks/FossilNonlinearReg/FossilPolyRegStan.html) and [quarto](https://github.com/mattiasvillani/BayesianLearningBook/raw/main/notebooks/FossilNonlinearReg/FossilPolyRegStan.qmd)<br>
Code: Getting started with Turing.jl: [instructions and code](code/Turing/turing_instructions.qmd) | [Survival analysis in Stan](https://mc-stan.org/docs/stan-users-guide/truncation-censoring.html)

### Exercises

Solutions for each problem can be folded out on the pages below (but try to solve it yourself first!).

+ [Exercise 2.1 - Math exercise on the posterior for exponentially distributed data](https://mattiasvillani.com/BayesianLearningBook/exercises/ch2solutions.html#sec-prob_post_exp)
+ [Exercise 2.2 - Math and computer exercise on exponential distribution for survival data](https://mattiasvillani.com/BayesianLearningBook/exercises/ch2solutions.html#sec-prob_post_exp_lung)
+ [Exercise 2.3 - Computer exercise on Weibull distribution for survival data](https://mattiasvillani.com/BayesianLearningBook/exercises/ch2solutions.html#sec-prob_post_weibull_lung)
+ [Exercise 7.2 - Normal posterior approximation for the Weibull survival model](https://mattiasvillani.com/BayesianLearningBook/exercises/ch7solutions.html#sec-prob_weibullreg_lung_optim)
+ [Exercise 7.3 - Normal posterior approximation for the Weibull survival regression model](https://mattiasvillani.com/BayesianLearningBook/exercises/ch7solutions.html#sec-prob_weibull_lung_optim)
+ [Exercise 10.1 - Posterior sampling from the posterior for the Weibull survival regression model using stan](https://mattiasvillani.com/BayesianLearningBook/exercises/ch10solutions.html#sec-prob_weibullreg_lung_stan)

### Extras <a name="extras"></a>

Interactive: [List of Bayesian learning widgets](https://observablehq.com/collection/@mattiasvillani/bayesian-learning) | [List of Statistical distributions widgets](https://observablehq.com/collection/@mattiasvillani/distributions)


### Computing

##### R and RStan
<img src="misc/R.png" width="25"> [Install R](https://cran.r-project.org/bin/)<br>
<img src="misc/RStudio.png" width="25"> [Install RStudio](https://posit.co/download/rstudio-desktop/)<br>
<img src="misc/rstan.png" width="25"> [Install RStan](https://github.com/stan-dev/rstan/wiki/Rstan-Getting-Started) | [Stan User's guide](https://mc-stan.org/docs/stan-users-guide/)

##### Julia and Turing
<img src="misc/julialogo.svg" width="25"> [Install Julia](https://julialang.org/downloads/)<br>
<img src="misc/vscodeicon.png" width="25"> [Install VS Code](https://code.visualstudio.com/)<br>
<img src="misc/vsjulia.png" width="25"> [Install Julia extension for VS Code](https://code.visualstudio.com/docs/languages/julia) | [Guided Youtube video](https://www.youtube.com/watch?v=FcgIvWb7gO0&ab_channel=doggodotjl)<br>
<img src="misc/turinglogo.jpg" width="25"> [Install and get started with Turing.jl](https://turing.ml/v0.22/docs/using-turing/get-started) | [Turing tutorials](https://turing.ml/v0.22/tutorials/)

