[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Bayes - a one day workshop",
    "section": "",
    "text": "Star\n\nAim\nThe aim of this one-day workshop is to introduce:\n\nthe basics of Bayesian statistics though some simple models\nthe Bayesian approach to prediction and decision making\napproximation and sampling algorithms for posterior inference\nBayesian regression and classification\nprobabilistic programming in Stan and Turing that will allow the user to tackle serious real-world problems with ease.\n\nThe treatment of each topic will necessarily be more brief than I would like to, but the partipants can dig deeper by:\n\nreading in the book suggested below\nexperiment with the extra material provided below\nfollow along in the material for my two courses:\n\nBayesian Learning. 7.5 credits\nAdvanced Bayesian Learning, 8 credits\n\n\n\n\nLecturer\n\nMattias Villani Professor of Statistics Stockholm University\n\n\nLiterature\n\nVillani, M. (2025). Bayesian Learning (draft, later chapters are work in progress)\n\n\n\nWorkshop plan and schedule\n\nLecture 1 - The Bayesics Time: 9.00-9.50  Reading: Ch. 1, 2.1-2.4 | Slides  Interactive: Beta distribution | Bernoulli data - beta prior | Gaussian known variance | Poisson model | Exponential model | Credible intervals\nLecture 2 - Multi-parameter models, Marginalization, Prior elicitation and Prediction Time: 10.00-10.50  Reading: Ch. 3.1-3.5, Ch.4 and Ch. 6 | Slides Interactive: Scaled inverse-chi2 distribution | Gaussian model | Dirichlet distribution | Multinomial model | Prior predictive Poisson model\n‚òï coffee\nLecture 3 - Bayesian Regression and Regularization Time: 11.10-12.00  Reading: Ch. 5 and Ch. 12 | Slides Interactive: Linear regression\nüç≤ lunch\nLecture 4 - Bayesian Classification and Posterior Approximation Time: 13.00-13.50  Reading: Ch. 7 and Ch. 8 | Slides Interactive: Beta regression for proportions Notebook: Logistic regression Titanic data in R: html and quarto\nLecture 5 - Introduction to Gibbs sampling, MCMC and HMC Time: 14.00-14.50  Reading: Ch. 9 and Ch. 10 (very incomplete) | Bayesian Data Analysis Ch. 10-11 and 12.4 | Slides Interactive: Random walk Metropolis | HMC | Leapfrog integrator | HMC on multimodal posterior Videos: Gibbs sampling when parameters are very correlated | Metropolis-Hastings vs HMC\n‚òï coffee\nLecture 6 - Implementing Bayesian Learning with Probabilistic Programming Time: 15.10-16.00  Reading: Ch. 1, 2.1-2.4 | Slides Notebooks: Polynomial regression for fossil data in Rstan html and quarto Code: Getting started with Turing.jl: instructions and code | Survival analysis in Stan\n\n\nExercises\nSolutions for each problem can be folded out on the pages below (but try to solve it yourself first!).\n\nExercise 2.1 - Math exercise on the posterior for exponentially distributed data\nExercise 2.2 - Math and computer exercise on exponential distribution for survival data\nExercise 2.3 - Computer exercise on Weibull distribution for survival data\nExercise 7.2 - Normal posterior approximation for the Weibull survival model - TBA\nNormal posterior approximation for the Exponential survival regression model - TBA\nNormal posterior approximation for the Weibull survival regression model - TBA\nStan to sample from the posterior for the Weibull survival regression model - TBA\n\n\n\nExtras \nInteractive: List of Bayesian learning widgets | List of Statistical distributions widgets\n\n\nComputing\n\nR and RStan\n Install R  Install RStudio  Install RStan | Stan User‚Äôs guide\n\n\nJulia and Turing\n Install Julia  Install VS Code  Install Julia extension for VS Code | Guided Youtube video  Install and get started with Turing.jl | Turing tutorials"
  },
  {
    "objectID": "code/logisticreg_titanic.html",
    "href": "code/logisticreg_titanic.html",
    "title": "Bayesian logistic regression for the Titanic data",
    "section": "",
    "text": "Load packages\n\nlibrary(mvtnorm)      # package with multivariate normal density\nlibrary(latex2exp)    # latex maths in plots\n\n\n\nSettings\n\ntau &lt;- 10             # Prior std beta~N(0,tau^2*I)\n\n\n\nRead data and set up prior\n\ndf &lt;- read.csv(\"https://github.com/mattiasvillani/introbayes/raw/main/data/titanic.csv\", header=TRUE) \ny &lt;- as.vector(df[,1])\nX &lt;- df[,c(5, 4, 2)]\nX[, 2] &lt;- 1*(X[,2] == \"female\")\nX[, 3] &lt;- 1*(X[,3] == 1)\nX &lt;- as.matrix(cbind(1,X))\np &lt;- dim(X)[2]\nvarNames = c(\"intercept\", \"age\", \"sex\", \"firstclass\")\n\n# Setting up the prior\nmu &lt;- as.vector(rep(0,p)) # Prior mean vector\nSigma &lt;- tau^2*diag(p)\n\n\nCoding up the log posterior function\n\nLogPostLogistic &lt;- function(betaVect, y, X, mu, Sigma){\n  p &lt;- length(betaVect)\n  linPred &lt;- X%*%betaVect\n  logLik &lt;- sum( linPred*y - log(1 + exp(linPred)))\n  logPrior &lt;- dmvnorm(betaVect, matrix(0,p,1), Sigma, log=TRUE)\n  return(logLik + logPrior)\n}\n\n\n\n\nFinding the mode and observed information using optim\n\ninitVal &lt;- as.vector(rep(0,p)); \nOptimResults&lt;-optim(initVal, LogPostLogistic, gr=NULL, y, X, mu, Sigma,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\n\n\n\nPlot the marginal posterior of \\(\\beta\\)\n\npar(mfrow=c(2,2))\nfor (j in 1:4){\n  gridVals = seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], \n                 length = 100)\n  plot(gridVals, dnorm(gridVals, mean = postMode[j], sd = postStd[j]), \n    xlab = TeX(sprintf(r'($\\beta_%d$)', j-1)), ylab= \"posterior density\", \n    type =\"l\", bty = \"n\", lwd = 2, col = \"cornflowerblue\", main =varNames[j])\n}\n\n\n\n\n\n\nPlot the marginal posterior of the odds \\(\\exp(\\beta)\\)\n\npar(mfrow=c(2,2)) \nfor (j in 1:4){\n  gridVals = exp(seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], length = 100))\n  plot(gridVals, dlnorm(gridVals, meanlog = postMode[j], sdlog = postStd[j]), \n    xlab = TeX(sprintf(r'($\\exp(\\beta_%d$))', j-1)), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n    lwd = 2, col = \"cornflowerblue\", main = varNames[j])\n}"
  },
  {
    "objectID": "code/NonlinearRegressionStan.html",
    "href": "code/NonlinearRegressionStan.html",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "",
    "text": "Model\nThis notebook illustrate how to use a polynomial regression model \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p + \\varepsilon, \\quad \\varepsilon \\overset{\\mathrm{iid}}{\\sim} N(0,\\sigma^2)\\] in rstan.\nPrior\nAn L2-prior (ridge) is used to prevent overfitting \\[\\beta_j \\vert \\sigma^2 \\overset{\\mathrm{iid}}{\\sim} N\\Big(0,\\frac{\\sigma^2}{\\lambda^2}\\Big)  \\] where the regularization parameter \\(\\lambda\\) (which is a precision = 1/variance parameter) is learned from the data. The regularization parameter is parameterized as variance \\(\\psi^2 = 1/\\lambda\\) in the sampling with the prior \\[ \\psi^2 \\sim \\mathrm{Inv-}\\chi^2(\\omega_0,\\psi_0^2).\\] The intercept is given a separate prior \\[\\beta_0 \\sim N(0,\\sigma_{0,\\mathrm{intercept}}^2)\\] The noise variance is assign the usual scaled inverse chi-squared prior \\[ \\sigma^2 \\sim \\mathrm{Inv-}\\chi^2(\\nu_0,\\sigma_0^2).\\]"
  },
  {
    "objectID": "code/NonlinearRegressionStan.html#footnotes",
    "href": "code/NonlinearRegressionStan.html#footnotes",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChaudhuri, P. and J. S. Marron (1999). Sizer for exploration of structures in curves. Journal of the American Statistical Association‚Ü©Ô∏é"
  },
  {
    "objectID": "code/FossilNonlinearRegStan.html",
    "href": "code/FossilNonlinearRegStan.html",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "",
    "text": "Model\nThis notebook illustrate how to use a polynomial regression model \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p + \\varepsilon, \\quad \\varepsilon \\overset{\\mathrm{iid}}{\\sim} N(0,\\sigma^2)\\] in rstan.\nPrior\nAn L2-prior (ridge) is used to prevent overfitting \\[\\beta_j \\vert \\sigma^2 \\overset{\\mathrm{iid}}{\\sim} N\\Big(0,\\frac{\\sigma^2}{\\lambda^2}\\Big)  \\] where the regularization parameter \\(\\lambda\\) (which is a precision = 1/variance parameter) is learned from the data. The regularization parameter is parameterized as variance \\(\\psi^2 = 1/\\lambda\\) in the sampling with the prior \\[ \\psi^2 \\sim \\mathrm{Inv-}\\chi^2(\\omega_0,\\psi_0^2).\\] The intercept is given a separate prior \\[\\beta_0 \\sim N(0,\\sigma_{0,\\mathrm{intercept}}^2)\\] The noise variance is assign the usual scaled inverse chi-squared prior \\[ \\sigma^2 \\sim \\mathrm{Inv-}\\chi^2(\\nu_0,\\sigma_0^2).\\]"
  },
  {
    "objectID": "code/FossilNonlinearRegStan.html#footnotes",
    "href": "code/FossilNonlinearRegStan.html#footnotes",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChaudhuri, P. and J. S. Marron (1999). Sizer for exploration of structures in curves. Journal of the American Statistical Association‚Ü©Ô∏é"
  },
  {
    "objectID": "code/Turing/turing_instructions.html",
    "href": "code/Turing/turing_instructions.html",
    "title": "Getting started with Turing.jl",
    "section": "",
    "text": "Here is what you need to do in order to run the code:\n\nDownload Julia and install it. I used version 1.9.2.\nDownload my turing.zip with the code and extract it to a folder and open a terminal in that folder.\nStart Julia by typing julia in the terminal.\nIn Julia, type ] and ENTER to enter the package manager. The prompt should change to something with pkg&gt;.\nactivate the environment by typing activate . and ENTER. (that‚Äôs activate followed by space and a dot).\nInstantiate the environment with all dependencies by typing instantiate and ENTER.\nPress BACKSPACE to exit the package manager.\nType for example include(\"iidnormalturing.jl\") and ENTER to run the code. After the sampling, a plot should appear.\n\nIf you get serious with Julia and want to use it for your own work, I recommend that you use an IDE such as VS Code with the Julia extension."
  }
]