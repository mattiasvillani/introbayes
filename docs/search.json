[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Bayes - a one day workshop",
    "section": "",
    "text": "Aim\nThe aim of this one-day workshop is to introduce:\n\nthe basics of Bayesian statistics though some simple models\nthe Bayesian approach to prediction and decision making\napproximation and sampling algorithms for posterior inference\nBayesian regression and classification\nprobabilistic programming in Stan and Turing that will allow the user to tackle serious real-world problems with ease.\n\nThe treatment of each topic will necessarily be more brief than I would like to, but the partipants can dig deeper by:\n\nreading in the book suggested below\nexperiment with the extra material provided below\nfollow along in the material for my two courses:\n\nBayesian Learning. 7.5 credits\nAdvanced Bayesian Learning, 8 credits\n\n\n\n\nLecturer\n\nMattias Villani Professor of Statistics Stockholm University\n\n\nLiterature\n\nVillani, M. (2025). Bayesian Learning (draft, later chapters are work in progress)\n\n\n\nWorkshop plan and schedule\n\nLecture 1 - The Bayesics Time: 9.00-9.50  Reading: Ch. 1, 2.1-2.4 | Slides  Interactive: Beta distribution | Bernoulli data - beta prior | Gaussian known variance | Poisson model | Exponential model | Credible intervals\nLecture 2 - Multi-parameter models, Marginalization, Prior elicitation and Prediction Time: 10.00-10.50  Reading: Ch. 3.1-3.5, Ch.4 and Ch. 6 | Slides Interactive: Scaled inverse-chi2 distribution | Gaussian model | Dirichlet distribution | Multinomial model | Prior predictive Poisson model\n‚òï coffee\nLecture 3 - Bayesian Regression and Regularization Time: 11.10-12.00  Reading: Ch. 5 and Ch. 12 | Slides Interactive: Linear regression\nüç≤ lunch\nLecture 4 - Bayesian Classification and Posterior Approximation Time: 13.00-13.50  Reading: Ch. 7 and Ch. 8 | Slides\nInteractive:\nLecture 5 - Introduction to Gibbs sampling, MCMC and HMC Time: 14.00-14.50  Reading: Ch. 9 and Ch. 10 (very incomplete) | Slides Interactive: Gibbs sampling when parameters are very correlated | Metropolis-Hastings vs HMC\n‚òï coffee\nLecture 6 - Implementing Bayesian Learning with Probabilistic Programming Time: 15.10-16.00  Reading: Ch. 1, 2.1-2.4 | Slides\n\n\nExtras \nInteractive: Bayes‚Äô theorem for events | Frequentist coverage of credible intervals | Normal approximation for Beta model | Bayesian hypothesis testing"
  }
]