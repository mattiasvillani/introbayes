[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Bayes - a one day workshop",
    "section": "",
    "text": "Aim\nThe aim of this one-day workshop is to introduce:\n\nthe basics of Bayesian statistics though some simple models\nthe Bayesian approach to prediction and decision making\napproximation and sampling algorithms for posterior inference\nBayesian regression and classification\nprobabilistic programming in Stan and Turing that will allow the user to tackle serious real-world problems with ease.\n\nThe treatment of each topic will necessarily be more brief than I would like to, but the partipants can dig deeper by:\n\nreading in the book suggested below\nexperiment with the extra material provided below\nfollow along in the material for my two courses:\n\nBayesian Learning. 7.5 credits\nAdvanced Bayesian Learning, 8 credits\n\n\n\n\nLecturer\n\nMattias Villani Professor of Statistics Stockholm University\n\n\nLiterature\n\nVillani, M. (2025). Bayesian Learning (draft, later chapters are work in progress)\n\n\n\nWorkshop plan and schedule\n\nLecture 1 - The Bayesics Time: 9.00-9.50  Reading: Ch. 1, 2.1-2.4 | Slides  Interactive: Beta distribution | Bernoulli data - beta prior | Gaussian known variance | Poisson model | Exponential model | Credible intervals\nLecture 2 - Multi-parameter models, Marginalization, Prior elicitation and Prediction Time: 10.00-10.50  Reading: Ch. 3.1-3.5, Ch.4 and Ch. 6 | Slides Interactive: Scaled inverse-chi2 distribution | Gaussian model | Dirichlet distribution | Multinomial model | Prior predictive Poisson model\n‚òï coffee\nLecture 3 - Bayesian Regression and Regularization Time: 11.10-12.00  Reading: Ch. 5 and Ch. 12 | Slides Interactive: Linear regression\nüç≤ lunch\nLecture 4 - Bayesian Classification and Posterior Approximation Time: 13.00-13.50  Reading: Ch. 7 and Ch. 8 | Slides Notebook: Logistic regression Titanic data in R: html and quarto\nLecture 5 - Introduction to Gibbs sampling, MCMC and HMC Time: 14.00-14.50  Reading: Ch. 9 and Ch. 10 (very incomplete) | Slides Interactive: Gibbs sampling when parameters are very correlated | Metropolis-Hastings vs HMC\n‚òï coffee\nLecture 6 - Implementing Bayesian Learning with Probabilistic Programming Time: 15.10-16.00  Reading: Ch. 1, 2.1-2.4 | Slides\n\n\nExtras \nInteractive: Bayes‚Äô theorem for events | Frequentist coverage of credible intervals | Normal approximation for Beta model | Bayesian hypothesis testing"
  },
  {
    "objectID": "code/logisticreg_titanic.html",
    "href": "code/logisticreg_titanic.html",
    "title": "Bayesian logistic regression for the Titanic data",
    "section": "",
    "text": "Load packages\n\nlibrary(mvtnorm)      # package with multivariate normal density\nlibrary(latex2exp)    # latex maths in plots\n\n\n\nSettings\n\ntau &lt;- 10             # Prior std beta~N(0,tau^2*I)\n\n\n\nRead data and set up prior\n\ndf &lt;- read.csv(\"https://github.com/mattiasvillani/introbayes/raw/main/data/titanic.csv\", header=TRUE) \ny &lt;- as.vector(df[,1])\nX &lt;- df[,c(5, 4, 2)]\nX[, 2] &lt;- 1*(X[,2] == \"female\")\nX[, 3] &lt;- 1*(X[,3] == 1)\nX &lt;- as.matrix(cbind(1,X))\np &lt;- dim(X)[2]\n\n# Setting up the prior\nmu &lt;- as.vector(rep(0,p)) # Prior mean vector\nSigma &lt;- tau^2*diag(p)\n\n\nCoding up the log posterior function\n\nLogPostLogistic &lt;- function(betaVect, y, X, mu, Sigma){\n  p &lt;- length(betaVect)\n  linPred &lt;- X%*%betaVect\n  logLik &lt;- sum( linPred*y - log(1 + exp(linPred)))\n  logPrior &lt;- dmvnorm(betaVect, matrix(0,p,1), Sigma, log=TRUE)\n  return(logLik + logPrior)\n}\n\n\n\n\nFinding the mode and observed information using optim\n\ninitVal &lt;- as.vector(rep(0,p)); \nOptimResults&lt;-optim(initVal, LogPostLogistic, gr=NULL, y, X, mu, Sigma,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\n\n\n\nPlot the marginal posterior of \\(\\beta\\)\n\npar(mfrow=c(2,2))\nfor (j in 1:4){\n  gridVals = seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], \n                 length = 100)\n  plot(gridVals, dnorm(gridVals, mean = postMode[j], sd = postStd[j]), \n    xlab = expression(beta[j]), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n    lwd = 2, col = \"cornflowerblue\", main = expression(beta[j]))\n}\n\n\n\n\n\n\nPlot the marginal posterior of the odds \\(\\exp(\\beta)\\)\n\npar(mfrow=c(2,2)) \nvarNames = c(\"intercept\", \"age\", \"sex\", \"firstclass\")\nfor (j in 1:4){\n  gridVals = exp(seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], length = 100))\n  plot(gridVals, dlnorm(gridVals, meanlog = postMode[j], sdlog = postStd[j]), \n    xlab = expression(exp(beta[j])), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n    lwd = 2, col = \"cornflowerblue\", main = varNames[j])\n}"
  }
]